<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>ARLearn</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheet/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheet/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">ARLearn</h1>
      <h2 class="project-tagline">Learning has never been more fun: A markerless AR based tutor for kids using deep learning</h2>
<!--       <a href="https://youtu.be/3LYjsKIqUAE" class="btn">Demo Video</a> -->
     
    </section>

    <section class="main-content">
      <h3>
<a id="welcome-to-arlearn" class="anchor" href="#welcome-to-arlearn" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome to ARLearn</h3>

<p>Animation and Graphics are extensively used to organise information, to foster insight, to facilitate memory and inference. To this end, we present an interactive and immersive augmented reality tutor for kids to learn alphabets and numbers on a hand-held device. 3D animations serve as a reward for construction of alphabets correctly.This engages children grabbing their attention and maintaining motivation during the learning process. The application is tested on subjects between ages 2 to 6 who were instructed to make models of alphabets/numbers with clay. The AR application starts by prompting the child with the alphabet shape to construct. Once the child constructs an alphabet using a clay model on a plain surface such as the table top, an image is captured by the application and sent to the recognition engine. Since children can create alphabets that could be slightly deformed - the proposed method utilises spatial transformer networks (STNs) with CNNs that work on alphabet recognition. It is robust and invariant to scaling, rotation, and affine transformations amongst others. The novelty is two fold in comparison with the state-of-the-art (a) by being generic in the sense that even though the demo video utilises clay models, it can be generalised to other alphabet learning methods: slate and a chalk, white paper and a color pencil/pen, white board and pen to name a few (b) by alleviating the need of explicit markers such as the flash cards to overlay the animation.</p>

<h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

<p><img src="https://github.com/rhebbalaguppe/ARLearn/tree/master/img/proposed-method.png?raw=true?" alt="Proposed Method "></p>

<p>We  present an  approach  for  marker-less  and  real-time touch-less gestures to mark ROI on wearables in FPV by proposing a novel two  stage  sequential  gesture  recognition  method as shown in the demo video.   First,  we  detect  a dynamic  gesture which involves detecting the presence of a stable hand , then raising the index finger with the rest of the fist closed(termed as point gesture) to trigger ROI Selection. This is followed by another dynamic gesture involving moving point gesture around the object of interest.   Our  approach  is  particularly  suitable  as  most  of  the smartphones available in the market are not equipped with built-in depth sensor posing additional challenges. The main blocks of the algorithm are: (i)point gesture detection, (ii) ROI selection, (iii) ROI tracking, and (iv) subsequent updating of bounding box around the ROI. Skin pixel detection followed by largest contour segmentation gives the hand region in the user's FOV. The fingertip is computed as the farthest point from the centroid of hand. After finger  tip  detection, a closed contour is drawn following the locus of the detected fingertip,  in the sequence of frames. After  the  ROI  is  selected,  the  resultant  bounding  box using an approach based on Shi-Tomasi feature points and optical flow vectors. The unreliable trajectories of the feature points are eliminated using the Forward-Backward Error method. 
We conducted experiments on  industrial which  demonstrates  that  the problem identification with our method of ROI highlighting gets faster by 60%, in comparison to sole audio instructions. </p>

<!-- <h3>
<a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/3LYjsKIqUAE" frameborder="0" allowfullscreen></iframe> -->
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/arc4224/AirGestures-ROI">TeleAssist-ROI</a> is maintained by <a href="https://github.com/arc4224">arc4224</a>.</span>

      </footer>

    </section>

  
  </body>
</html>
